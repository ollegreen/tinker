{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"your\":     0,\n",
    "    \"cat\":      1,\n",
    "    \"is\":       2,\n",
    "    \"a\":        3,\n",
    "    \"lovely\":   4,\n",
    "    \"my\":       5,\n",
    "    \"you\":      6,\n",
    "    \"olle\":     7,\n",
    "    \"matrea\":   8,\n",
    "    \"dad\":         9,\n",
    "    \"mom\":         10,\n",
    "    \"pet\":         11,\n",
    "    \"rosie\":         12,\n",
    "    \"fumble\":        13,\n",
    "    \"petrol\":         14,\n",
    "    \"tummy\":         15,\n",
    "    \"sage\":         16,\n",
    "    \"duck\":         17,\n",
    "    \"liver\":         18,\n",
    "    \"onion\":         19,\n",
    "    \"tomato\":         20,\n",
    "    \"pepper\":         21,\n",
    "    \"langchain\":         22,\n",
    "    \"are\":         23,\n",
    "    \"the\":         24,\n",
    "    \"best\":         25,\n",
    "    \"of\":         26,\n",
    "    \"all\":         27,\n",
    "    \"bum\":         28,\n",
    "    \"lol\":         29,\n",
    "    \"cool\":         30,\n",
    "    \"story\":         31,\n",
    "    \"man\":         32,\n",
    "    \"woman\":         33,\n",
    "    \"[UNK]\":    34  # Unknown token for words not in the vocabulary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Your\", \"cat\", \"is\", \"a\", \"lovely\", \"cat\", \"mate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.get(sentence[6].lower(), vocabulary[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 1, 34]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lowercase to match the vocabulary keys\n",
    "indices = [vocabulary.get(word.lower(), vocabulary[\"[UNK]\"]) for word in sentence]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  1, 34])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_indices = torch.tensor(indices, dtype=torch.long)\n",
    "input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # input dimensionality\n",
    "        self.vocab_size = vocab_size # the total number of different words that are in the model's vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # ^size of the dict of embeddings + embedding vector\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding(input) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model =  8 # dims of embeddings\n",
    "vocab_size = len(vocabulary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = InputEmbeddings(d_model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 20, 10])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9320,  0.2359, -0.1124,  6.3901, -1.9721,  5.6444, -3.2587,  4.3600],\n",
       "        [-2.6714, -0.8874,  4.7699, -1.6078,  0.0722,  0.8359, -0.2876, -2.8380],\n",
       "        [-5.4204, -1.2565, -3.0293,  5.3176,  1.3457,  5.7312, -1.9031,  4.4588],\n",
       "        [ 7.1102, -1.0455,  1.9530, -0.1062, -0.9734,  5.6528, -0.9597,  2.4300],\n",
       "        [-1.2621, -1.3150,  2.5110, -2.3639, -0.7236,  2.3629, -0.4232, -0.8170],\n",
       "        [-2.6714, -0.8874,  4.7699, -1.6078,  0.0722,  0.8359, -0.2876, -2.8380],\n",
       "        [ 2.4668, -1.3098,  0.6046, -2.8471,  1.0624, -4.8806,  1.7584,  1.4341]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = input_embedding(input_indices)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
